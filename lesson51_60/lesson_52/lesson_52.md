# lesson_52

---

# Observability & Cost Control (ASG + ALB)

**Date:** 2026-01-31

**Focus:** Visibility, alerts, and cost-aware operation of an auto-scaling system

---

## Why This Lesson Exists

Up to lesson_51:

- the system **reacts**
- scaling works
- instances are replaced correctly

From lesson_52 onward:

- the system **explains itself**
- detect problems *before* users do
- scaling decisions are **visible and cost-aware**

> A system you can't observe is a system you don't control.

---

## Target Architecture

```
Users
  |
  v
ALB -- metrics --> CloudWatch Dashboard
  |                   ^
  v                   |
ASG -- metrics -------+
  |
  v
EC2 instances (baked AMI)

```

Observability here is **AWS-native**, no Prometheus yet. This is **intentional**.

---

## Goals / Acceptance Criteria

- [ ] Know **which metrics matter** and why
- [ ] A CloudWatch dashboard shows ASG + ALB state
- [ ] Alarms fire on real failure signals
- [ ] Scaling is adjusted with **cost awareness**
- [ ] Can explain an incident **from metrics alone**

---

## Preconditions

Already have:

- ASG attached to ALB (lesson_50)
- Scaling policies working (lesson_51)
- Internal ALB reachable from proxy
- No direct EC2 management

If not, stop and fix that first.

---

## A) Metrics Baseline -- What Actually Matters

### A1) ALB Metrics (User Experience)

Namespace: `AWS/ApplicationELB`

**Core signals:**

- `RequestCount` -- traffic level
- `TargetResponseTime` -- latency
- `HTTPCode_ELB_5XX_Count` -- ALB failures
- `UnHealthyHostCount` -- broken capacity

---

`RequestCount`**:** how many requests actually arrived

**Why it matters:**

- this is *demand*, not guesswork
- CPU without traffic = noise

**Reality check:**

- RequestCount grows -> *external stimulus exists*
- CPU grows without RequestCount -> *problem inside the instance*

---

`TargetResponseTime`**:** backend latency, not the ALB

**Why it matters:**

- ALB can be green
- the app can already be dying

**Fact:**

Latency rises **before** 5XX -- an early signal.

---

`HTTPCode_ELB_5XX_Count`**:** errors generated by **the ALB**

**Why it matters:**

- this is already an **incident**, not a warning

**Correct idea:**

Errors must be **sustained**, not random.

---

`UnHealthyHostCount`**:** how many targets actually dropped out

**Why it matters:**

- this is **capacity loss**, not an abstraction

**Prod fail:**

"ASG has 4 instances, all good" -- ALB sees only 2 healthy -> you are already under load.

---

> *ALB is the only honest source of truth about users.*

*EC2 can lie. ASG can lie. ALB -- almost never.*

---

### A2) ASG Metrics (Capacity & Decisions)

Namespace: `AWS/AutoScaling`

- `GroupDesiredCapacity`
- `GroupInServiceInstances`
- `GroupMinSize`
- `GroupMaxSize`

---

`GroupDesiredCapacity`**:** how many instances the system *wants*

**Why it matters:**

- this is the **algorithm output**, not reality

---

`GroupInServiceInstances`**:** how many are actually ready to take traffic

**Why it matters:**

- this is **real capacity**

---

`Min / Max Size`

**Why it matters:**

- these are **financial and SLO boundaries**, not just numbers

---

> *ASG shows **intent and limits**, not health.*

---

### A3) EC2 Metrics (Pressure)

Namespace: `AWS/EC2`

- `CPUUtilization`
- `NetworkIn`
- `NetworkOut`

---

`CPUUtilization`**:** CPU load

**Why it matters:**

- this is the **cause**, not the result
- CPU is a **trigger**, not an **incident**

---

`NetworkIn / Out`

**Why it matters:**

- distinguishes real traffic from a busy loop

---

**EC2 metrics answer:**

> "Why did the system decide to scale?"

But **not**:

> "Are users hurting?"

---

### End of Section A (important)

| Question | Who answers |
| --- | --- |
| Are users hurting? | ALB |
| What did the system decide? | ASG |
| Why did it decide that? | EC2 |

---

## B) CloudWatch Dashboard (Single Pane of Glass)

A dashboard is not visualization.

It is an **investigation tool**. If you cannot explain an incident from it, it is useless.

### Dashboard Design Rule

One dashboard. Three rows. No clutter. Top to bottom = from users to metal.

```bash
[ ALB -- user impact ]
[ ASG -- decisions ]
[ EC2 -- pressure ]
```

---

## Row 1 -- ALB (User-facing, most important)

- **RequestCount**
- **TargetResponseTime**
- **HTTPCode_ELB_5XX_Count**

### Why this order

- RequestCount = *what arrived*
- ResponseTime = *how the system copes*
- 5XX = *users are hurting*

### Prod failures caught here

- latency rises **before** scale-out -> target CPU too high
- 5XX appear **without** traffic growth -> degradation, not load
- traffic is zero but CPU is 90% -> bug, not users

---

## Row 2 -- ASG (Control plane, the system's brain)

- **GroupDesiredCapacity**
- **GroupInServiceInstances**

### Why side by side

- Desired up, InService up -> healthy
- Desired up, InService stuck -> bad AMI / boot / health
- Desired down, InService down -> scale-in works
- Desired = InService, but ALB is bad -> scaling the wrong thing

The dashboard immediately shows:

- the system **wanted**
- but **could not**

---

## Row 3 -- EC2 (Pressure, cause of decisions)

### Metrics

- **Average CPUUtilization** (dimension: ASG)

### Why Average, not Max

Max:

- one sick instance
- noise
- false positives

Average:

- collective pressure
- correct trigger for Target Tracking

---

## Dashboard Settings (do not skip)

### Time range

- **Last 1 hour**

Why:

- you can see scale-out -> cooldown -> scale-in
- no historical noise

### Period

- **60 seconds**

Why:

- ASG and ALB think in minutes
- 5 seconds = false precision

---

## Typical mistakes

- 5+ dashboards -> you lose context
- mixing ALB and EC2 in one row -> you mix cause and effect
- "just in case" graphs -> during an incident you look at the wrong thing
- no Desired vs InService -> blind to boot failures

**Acceptance:**

- [ ] You see the scale-out moment and know **what caused it**
- [ ] You see instance replacement visually
- [ ] You can explain the graph **without logs or SSH**

---

## C) Alarms -- Signals, Not Noise (only what requires action)

### Alarm Philosophy

Alarms are for:

- **user impact (users are hurting)**
- **capacity loss (the system is losing ability to serve)**
- an alert without a concrete action = noise

Not for:

- every CPU spike
- transient noise
- noise kills trust; without trust, alerts get ignored

---

### C1) ALB 5XX Alarm (Critical)

```hcl
# ALB 5XX - critical signal.
resource "aws_cloudwatch_metric_alarm" "alb_5xx_critical" {
  alarm_name          = "${var.project_name}-alb-5xx-critical"
  comparison_operator = "GreaterThanOrEqualToThreshold"
  metric_name         = "HTTPCode_ELB_5XX_Count"
  namespace           = "AWS/ApplicationELB"
  statistic           = "Sum"   # count real errors
  period              = 60
  evaluation_periods  = 2       # errors must be sustained for >= 2 minutes
  threshold           = 5       # 5 errors in 2 minutes is not random
  treat_missing_data  = "notBreaching"

  dimensions = {
    LoadBalancer = aws_lb.app.arn_suffix
  }

  alarm_description = "ALB 5XX - critical signal"
}
```

### Prod failure

`evaluation_periods = 1` -> one hiccup = pager at 3am.

This is a **real incident signal**.

---

### C2) Unhealthy Targets Alarm

```hcl
resource "aws_cloudwatch_metric_alarm" "alb_unhealthy" {
  alarm_name          = "${var.project_name}-alb-unhealthy-hosts"
  comparison_operator = "GreaterThanThreshold"
  metric_name         = "UnHealthyHostCount"
  namespace           = "AWS/ApplicationELB"
  statistic           = "Average"           # Average > 0 = at least one dead target
  period              = 60
  evaluation_periods  = 1
  threshold           = 0
  treat_missing_data  = "notBreaching"

  dimensions = {
    LoadBalancer = aws_lb.app.arn_suffix
    TargetGroup  = aws_lb_target_group.web.arn_suffix
  }

  alarm_description = "ALB Unhealthy hosts - critical signal"
}
```

### Why evaluation = 1

Losing a target is an **immediate risk**:

- capacity down
- latency up
- scale-out may not catch up

---

### C3) Capacity Drift Alarm

**Meaning:**

> The system wants X but actually has Y.

Alert when:

- GroupDesiredCapacity != GroupInServiceInstances for > N minutes

This often signals:

- bad AMI
- nginx does not start
- broken health checks
- instance boot is too slow

## What a good alarm looks like

When it fires, you **immediately** know:

- is it user impact or capacity?
- is the system scaling?
- should you wait for auto-heal or intervene?

If you need SSH to understand *what even happened* -- the alarm is bad.

## What should NOT exist (and why)

### CPU alarms

- CPU is a cause, not a problem
- high CPU + low latency = everything is fine

### Network alarms

- too context dependent
- meaningless without user metrics

### 10+ alarms

- you will stop reading them

**Acceptance:**

- [ ] There are **exactly 2-3 alarms**
- [ ] Every alarm maps to a concrete action
- [ ] No "FYI" alarms

---

## D) Cost-Aware Scaling (Important Shift)

### Target Tracking != Optimal Cost

Example:

- CPU target = 50% -> safe, but expensive
- CPU target = 60-65% -> cheaper, still safe

**Practice:**

- increase target CPU from 50 -> 60
- observe:
    - fewer instances
    - acceptable latency

**Golden rule**

> Raise the target slowly and look at ALB, not CPU.

---

### Scheduled Scaling (Real Money Saver)

Night and morning scale example:

```hcl
# Scheduled action to scale down at 22:00 UTC (Ireland local time).
resource "aws_autoscaling_schedule" "scale_down_night" {
  scheduled_action_name  = "${var.project_name}-web-scale-down-night"
  autoscaling_group_name = aws_autoscaling_group.web.name
  desired_capacity       = 1
  min_size               = 1
  max_size               = 2
  start_time             = "2026-01-31T22:00:00Z"
  recurrence             = "0 22 * * *" # Every day at 22:00 UTC (Ireland local time)

}

# Scheduled action to scale up at 06:00 UTC (Ireland local time).
resource "aws_autoscaling_schedule" "scale_up_morning" {
  scheduled_action_name  = "${var.project_name}-web-scale-up-morning"
  autoscaling_group_name = aws_autoscaling_group.web.name
  desired_capacity       = 2
  min_size               = 2
  max_size               = 4
  start_time             = "2026-01-31T06:00:00Z"
  recurrence             = "0 6 * * *" # Every day at 06:00 UTC (Ireland local time)

}
```

### Why it works

- no traffic at night
- no ML
- **real money saved**

**Always in pairs**:

- scale-down night
- scale-up morning

---

## Acceptance

- [ ] You changed the CPU target and **saw** the effect
- [ ] Latency stayed within acceptable bounds
- [ ] There are fewer instances
- [ ] No alert fired

You should **see this on the graphs**.

---

## E) Drills: Observe -> Decide -> Verify

---

### Drill 1 -- Observe Scale-Out (load)

1. Generate load (through the proxy, as intended)

```bash
ab -t 300 -c 200 http://<internal-alb-dns>/

```

### Look in the **right order**

1. **ALB / RequestCount (Sum)** -> traffic actually arrived
2. **EC2 / CPUUtilization (Average)** -> pressure increased
3. **ASG / GroupDesiredCapacity** -> the system *made a decision*
4. **ASG / GroupInServiceInstances** -> the system *executed it*

**Acceptance:**

- [ ] You saw RequestCount grow
- [ ] CPU grew **after**, not "by itself"
- [ ] GroupDesiredCapacity up **before** InService
- [ ] GroupInService caught up with Desired

---

### Drill 2 -- Backend Failure

1. Break nginx on one instance:

```bash
sudo systemctl stop nginx

```

2. Observe:
- UnHealthyHostCount > 0
- 5XX errors appear
- Alarm fires

**Acceptance:**

- [ ] Alarm fired
- [ ] Root cause visible in metrics
- [ ] Recovery was automatic

---

### Drill 3 -- Cost Mode

1. Stop the load
2. Observe:
    - CPU down
    - DesiredCapacity down
    - InService down
3. ALB:
    - latency stable
    - 5XX = 0
    - unhealthy = 0

**Acceptance:**

- [ ] System shrinks quietly
- [ ] No false positives

---

## Key takeaway from the drills

You can now distinguish:

| Situation | What it is |
| --- | --- |
| CPU up, ALB OK | normal load |
| ALB 5XX | incident |
| UnHealthyHostCount up | capacity loss |
| Desired != InService | system degradation |
| Scale-in | not a problem |

---

## Final Acceptance

- [ ] Explain **any** graph on the dashboard
- [ ] Tell an incident story **from metrics**
- [ ] Change cost vs risk deliberately
- [ ] Trust the alerts
- [ ] Do not touch instances by hand

---

## Common Pitfalls

- Alarming on CPU instead of user impact
- Too many dashboards
- No cost thinking
- Trusting scaling without observing it
- "It scaled but I don't know why"

---

## Security Checklist

- No secrets in dashboards
- IAM-scoped access to CloudWatch
- Alarms tied to meaningful signals
- Observability does not expand attack surface

---

## Summary

lesson_52 is where scaling stops being a guess.

From here on:

- ALB is user truth
- ASG shows intent vs reality
- EC2 shows pressure
- alerts map to action, not noise
